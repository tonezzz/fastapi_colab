{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f256f7be",
   "metadata": {},
   "source": [
    "# Colab FastAPI + Ollama + YOLO\n",
    "\n",
    "\"\n",
    "            \"This notebook provisions the FastAPI project, installs Ollama/YOLO, and exposes the API via ngrok.\n",
    "\"\n",
    "            \"Run the cells top-to-bottom whenever you need a fresh Colab runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ae868",
   "metadata": {},
   "source": [
    "## Before you run\n",
    "\"\n",
    "            \"1. Upload this repository (or sync from Git) into `/content/colab_fastapi`.\n",
    "\"\n",
    "            \"2. (Optional) Add your ngrok authtoken to the Colab secrets manager and set `NGROK_AUTHTOKEN`.\n",
    "\"\n",
    "            \"3. Decide which Ollama + YOLO models you want to use; defaults are `phi3` and `yolov8n.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = Path('/content/colab_fastapi').resolve()\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise RuntimeError('Upload the project into /content/colab_fastapi before continuing.')\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print('Working directory:', PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault('FASTAPI_PORT', '8000')\n",
    "os.environ.setdefault('OLLAMA_MODEL', 'phi3')\n",
    "os.environ.setdefault('YOLO_MODEL', 'yolov8n.pt')\n",
    "os.environ.setdefault('YOLO_CONFIDENCE', '0.35')\n",
    "\n",
    "print('FASTAPI_PORT =', os.environ['FASTAPI_PORT'])\n",
    "print('OLLAMA_MODEL =', os.environ['OLLAMA_MODEL'])\n",
    "print('YOLO_MODEL =', os.environ['YOLO_MODEL'])\n",
    "print('YOLO_CONFIDENCE =', os.environ['YOLO_CONFIDENCE'])\n",
    "print('NGROK_AUTHTOKEN =', os.environ.get('NGROK_AUTHTOKEN', '<unset>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381157da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/colab_fastapi\n",
    "sudo apt-get update -y\n",
    "sudo apt-get install -y curl git\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/colab_fastapi\n",
    "chmod +x scripts/install_ollama.sh\n",
    "export OLLAMA_MODEL=${OLLAMA_MODEL:-phi3}\n",
    "export OLLAMA_PORT=${OLLAMA_PORT:-11434}\n",
    "scripts/install_ollama.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a78989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "FASTAPI_PORT = int(os.environ['FASTAPI_PORT'])\n",
    "\n",
    "if 'SERVER_THREAD' in globals():\n",
    "    print('FastAPI server already running.')\n",
    "else:\n",
    "    config = uvicorn.Config('app.main:app', host='0.0.0.0', port=FASTAPI_PORT, log_level='info')\n",
    "    server = uvicorn.Server(config)\n",
    "\n",
    "    def _run_server():\n",
    "        asyncio.run(server.serve())\n",
    "\n",
    "    SERVER_THREAD = threading.Thread(target=_run_server, daemon=True)\n",
    "    SERVER_THREAD.start()\n",
    "    time.sleep(3)\n",
    "    print(f'FastAPI server started on port {FASTAPI_PORT}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c77e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from scripts.start_ngrok import start_ngrok\n",
    "\n",
    "public_url = start_ngrok(port=int(os.environ['FASTAPI_PORT']), authtoken=os.environ.get('NGROK_AUTHTOKEN'))\n",
    "public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import httpx\n",
    "import numpy as np\n",
    "\n",
    "FASTAPI_ROOT = f\"http://127.0.0.1:{os.environ['FASTAPI_PORT']}\"\n",
    "\n",
    "async def run_smoke_tests():\n",
    "    async with httpx.AsyncClient(timeout=120) as client:\n",
    "        gen_payload = {\n",
    "            'prompt': 'Say hello from the Colab FastAPI service.',\n",
    "            'model': os.environ.get('OLLAMA_MODEL'),\n",
    "        }\n",
    "        gen_resp = await client.post(f'{FASTAPI_ROOT}/ollama/generate', json=gen_payload)\n",
    "        gen_resp.raise_for_status()\n",
    "        print('Ollama /generate →', gen_resp.json())\n",
    "\n",
    "        dummy = np.zeros((320, 320, 3), dtype=np.uint8)\n",
    "        cv2.putText(dummy, 'COLAB', (30, 170), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
    "        success, buffer = cv2.imencode('.jpg', dummy)\n",
    "        if not success:\n",
    "            raise RuntimeError('Failed to encode dummy image.')\n",
    "        files = {'file': ('dummy.jpg', buffer.tobytes(), 'image/jpeg')}\n",
    "        detect_resp = await client.post(f'{FASTAPI_ROOT}/yolo/detect', files=files)\n",
    "        detect_resp.raise_for_status()\n",
    "        print('YOLO /detect →', detect_resp.json())\n",
    "\n",
    "asyncio.run(run_smoke_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343d700",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Run the cell below when you need to stop the server and ngrok tunnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "if 'server' in globals():\n",
    "    asyncio.run(server.shutdown())\n",
    "    print('FastAPI server shutdown requested.')\n",
    "if 'SERVER_THREAD' in globals():\n",
    "    SERVER_THREAD.join(timeout=5)\n",
    "    print('Server thread joined.')\n",
    "\n",
    "from pyngrok import ngrok\n",
    "ngrok.kill()\n",
    "print('ngrok tunnel closed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
